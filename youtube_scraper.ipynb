{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smitha-nair/abcd/blob/main/youtube_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ablX9AHyaVOt",
        "outputId": "81640a2f-c212-4451-adc7-aa44e24b48c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube-comment-downloader\n",
            "  Downloading youtube_comment_downloader-0.1.76-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting dateparser (from youtube-comment-downloader)\n",
            "  Downloading dateparser-1.2.0-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-comment-downloader) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from dateparser->youtube-comment-downloader) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateparser->youtube-comment-downloader) (2024.2)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.10/dist-packages (from dateparser->youtube-comment-downloader) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser->youtube-comment-downloader) (5.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-comment-downloader) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-comment-downloader) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-comment-downloader) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-comment-downloader) (2024.12.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->dateparser->youtube-comment-downloader) (1.17.0)\n",
            "Downloading youtube_comment_downloader-0.1.76-py3-none-any.whl (8.2 kB)\n",
            "Downloading dateparser-1.2.0-py2.py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dateparser, youtube-comment-downloader\n",
            "Successfully installed dateparser-1.2.0 youtube-comment-downloader-0.1.76\n"
          ]
        }
      ],
      "source": [
        "#to download comments from YouTube videos\n",
        "!pip install youtube-comment-downloader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "5SeiWHbGbWV5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytb_video_list = ['https://youtu.be/0sOvCWFmrtA']\n",
        " #             'https://www.youtube.com/watch?v=TuIgtitqJho']\n",
        "#                  'https://www.youtube.com/watch?v=hinZO--TEk4']"
      ],
      "metadata": {
        "id": "qG9lYXIubWYH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lZb-wO-p1Ame"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6eEhVCcH01VR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vid_id_separator(ytb_video_list):\n",
        "  list1=[]\n",
        "  for item in ytb_video_list:\n",
        "    if re.search('youtu.be',item):\n",
        "      item_list=item.split('/')\n",
        "      print(\"item_list\",item_list)\n",
        "      list1.append(item_list[3])\n",
        "    if re.search('www.youtube.com',item):\n",
        "      item_list=item.split('v=')\n",
        "      list1.append(item_list[1])\n",
        "  #print(\"list1....\",list1)\n",
        "  return list1\n",
        "def cmt_dow(list1):\n",
        "  for item in list1:\n",
        "    print(item)\n",
        "    !youtube-comment-downloader --youtubeid {item} --output {item}.json\n",
        "def csv_ext(id_list):\n",
        "  #print(\"id_list....\",id_list)\n",
        "  df=pd.DataFrame()\n",
        " # df = pd.DataFrame(columns=list(\"Text\"))\n",
        "\n",
        "  #print(\"df ........\",df)\n",
        "  fname=[]\n",
        "  for item in id_list:\n",
        "    ll='/content/'+item+'.json'\n",
        "    fname.append(ll)\n",
        "  #print(\"fname...\",fname)\n",
        "  for fname in fname:\n",
        "    with open(fname, \"r\") as a_file:\n",
        "      for line in a_file:\n",
        "   #     print(\"line...\",line)\n",
        "        res = json.loads(line) #to deserialize a JSON string (text) into a Python dictionary or list.\n",
        "        list_s=[res['text']]\n",
        "  #      print(\"list_s....\",list_s)\n",
        "  #      print(\"type of list_s\", type(list_s))\n",
        "  #      df= df.append(pd.DataFrame([list_s], columns=[\"Text\"]), ignore_index=True)\n",
        "        df =  pd.concat([df, pd.DataFrame(list_s)], ignore_index=True)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "TMTM4jEjPNVy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final_function(ytb_video_list):\n",
        "  inp1=vid_id_separator(ytb_video_list)\n",
        "  print(\"inp1...\",inp1)\n",
        "  cmt_dow(inp1)\n",
        "  inp3=csv_ext(inp1)\n",
        "  print(\"inp3....\",inp3)\n",
        "  return inp3"
      ],
      "metadata": {
        "id": "tZ7xrYZucICz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1=final_function(ytb_video_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "802qpLVMluAZ",
        "outputId": "8624dc26-7419-416a-f0d9-ff88d7a5ab79"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "item_list ['https:', '', 'youtu.be', '0sOvCWFmrtA']\n",
            "inp1... ['0sOvCWFmrtA']\n",
            "0sOvCWFmrtA\n",
            "Downloading Youtube comments for 0sOvCWFmrtA\n",
            "Downloaded 2318 comment(s)\n",
            "[98.86 seconds] Done!\n",
            "inp3....                                                       0\n",
            "0     This is the 1st vedio I saw from your channel ...\n",
            "1     Can't deploy at heroku. Maybe Credit card issu...\n",
            "2     from app import models, schemas, utils this is...\n",
            "3     4:31:10 No Diddy \\n\\n\\n\\n\\nJokes aside, great ...\n",
            "4     2:53:10 I'd recommend DevDocs, as it makes doc...\n",
            "...                                                 ...\n",
            "2313  Do police in Nigeria takes bribe for no real r...\n",
            "2314                                   Finally First!!!\n",
            "2315                                              First\n",
            "2316                                                ðŸ‘ŒðŸ»ðŸ”¥\n",
            "2317                                                 â¤ï¸\n",
            "\n",
            "[2318 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1)\n",
        "# Define the file path for the Excel file\n",
        "excel_file_path = 'data.xlsx'\n",
        "\n",
        "# Convert the DataFrame to an Excel file\n",
        "df1.to_excel(excel_file_path, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abwEDzmgB6w6",
        "outputId": "5c02bf1b-5b23-4517-bfc6-c302e2c09bfa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                      0\n",
            "0     This is the 1st vedio I saw from your channel ...\n",
            "1     Can't deploy at heroku. Maybe Credit card issu...\n",
            "2     from app import models, schemas, utils this is...\n",
            "3     4:31:10 No Diddy \\n\\n\\n\\n\\nJokes aside, great ...\n",
            "4     2:53:10 I'd recommend DevDocs, as it makes doc...\n",
            "...                                                 ...\n",
            "2313  Do police in Nigeria takes bribe for no real r...\n",
            "2314                                   Finally First!!!\n",
            "2315                                              First\n",
            "2316                                                ðŸ‘ŒðŸ»ðŸ”¥\n",
            "2317                                                 â¤ï¸\n",
            "\n",
            "[2318 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "#downloads the VADER (Valence Aware Dictionary and sEntiment Reasoner) lexicon dataset. VADER is a lexicon and\n",
        "# rule-based sentiment analysis tool specifically designed for analyzing sentiments expressed in text.\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load the Excel file\n",
        "df = pd.read_excel('data.xlsx')  # Replace 'your_file.xlsx' with the path to your Excel file\n",
        "\n",
        "# Initialize the sentiment analyzer\n",
        "#performing sentiment analysis using the VADER (Valence Aware Dictionary and sEntiment Reasoner)\n",
        "# lexicon and rule-based sentiment analysis tool.\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Perform sentiment analysis and classification for each text entry\n",
        "sentiments = []\n",
        "for index, row in df.iterrows():\n",
        "    text = str(row['text'])  # Replace 'text_column' with the name of the column containing text data\n",
        "    sentiment_score = sid.polarity_scores(text)\n",
        "\n",
        "#When we create a SentimentIntensityAnalyzer object, we initialize an instance of the sentiment analyzer.\n",
        "#This analyzer is pre-trained with the VADER lexicon, which contains sentiment scores for words and phrases.\n",
        "#The polarity_scores() method of the SentimentIntensityAnalyzer  class\n",
        "# takes a piece of text as input and returns a dictionary of sentiment scores, including:\n",
        "# Classify sentiment\n",
        "#Negative: Negative sentiment intensity score\n",
        "#Neutral: Neutral sentiment intensity score\n",
        "#Positive: Positive sentiment intensity score\n",
        "#Compound: Compound sentiment score, which is a normalized weighted composite score calculated\n",
        "# based on the individual sentiment scores. It ranges from -1 (extremely negative) to +1 (extremely positive).\n",
        "\n",
        "    if sentiment_score['compound'] >= 0.05:\n",
        "        sentiment_class = \"Positive\"\n",
        "    elif sentiment_score['compound'] <= -0.05:\n",
        "        sentiment_class = \"Negative\"\n",
        "    else:\n",
        "        sentiment_class = \"Neutral\"\n",
        "\n",
        "    # Append the sentiment analysis results to the list\n",
        "    sentiments.append({'Text': text, 'Sentiment': sentiment_score, 'Sentiment_Class': sentiment_class})\n",
        "\n",
        "# Convert the list of sentiment analysis results to a DataFrame\n",
        "sentiments_df = pd.DataFrame(sentiments)\n",
        "\n",
        "# Save the results to a new Excel file or perform further analysis\n",
        "sentiments_df.to_excel('sentiment_analysis_results.xlsx', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "B6tNsK1EhX1k",
        "outputId": "f2626920-9a24-4699-bd21-f272ca166508"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1458c5775a9b>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0msentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace 'text_column' with the name of the column containing text data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0msentiment_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;31m# Convert generator to list before going through hashable part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load the Excel file\n",
        "df = pd.read_excel('data.xlsx')\n",
        "\n",
        "# Initialize the sentiment analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Perform sentiment analysis and classification for each text entry\n",
        "sentiments = []\n",
        "for index, row in df.iterrows():\n",
        "    # Accessing the first column by index instead of by name 'text'\n",
        "    text = str(row[0])\n",
        "    sentiment_score = sid.polarity_scores(text)\n",
        "\n",
        "    if sentiment_score['compound'] >= 0.05:\n",
        "        sentiment_class = \"Positive\"\n",
        "    elif sentiment_score['compound'] <= -0.05:\n",
        "        sentiment_class = \"Negative\"\n",
        "    else:\n",
        "        sentiment_class = \"Neutral\"\n",
        "\n",
        "    # Append the sentiment analysis results to the list\n",
        "    sentiments.append({'Text': text, 'Sentiment': sentiment_score, 'Sentiment_Class': sentiment_class})\n",
        "\n",
        "# Convert the list of sentiment analysis results to a DataFrame\n",
        "sentiments_df = pd.DataFrame(sentiments)\n",
        "\n",
        "# Save the results to a new Excel file or perform further analysis\n",
        "sentiments_df.to_excel('sentiment_analysis_results.xlsx', index=False)"
      ],
      "metadata": {
        "id": "rMyREAB6sLsA",
        "outputId": "10d37fc2-952a-4755-cb2e-1481c58393f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}